# Simulator

### PIX2NVS: Parameterized conversion of pixel-domain video frames to neuromorphic vision streams
Y Bi, Y Andreopoulos - 2017 IEEE International Conference on Image Processing (ICIP), 2017

We propose and make available a generic pixel-to-neuromorphic vision stream (PIX2NVS) framework in order to allow for the generation of neuromorphic data streams from conventional pixel-domain video frames. In order to quantify the accuracy of our framework against experimentally-derived NVS data from previous work, we also propose and validate two metrics, the Chamfer distance and e-repeatability. The most important application of PIX2NVS will be in the generation of artificial NVS from large annotated video frame collections used in machine learning research, e.g., YouTube-8M, YFCC100m, YouTube-BoundingBoxes, thereby transferring these datasets to the neuromorphic domain.

### Video to events: Recycling video datasets for event cameras

D Gehrig, M Gehrig, J Hidalgo-Carrió, D Scaramuzza - Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020
Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous" events" instead of intensity frames. They offer significant advantages with respect to conventional cameras: high dynamic range (HDR), high temporal resolution, and no motion blur. Recently, novel learning approaches operating on event data have achieved impressive results. Yet, these methods require a large amount of event data for training, which is hardly available due the novelty of event sensors in computer vision research. In this paper, we present a method that addresses these needs by converting any existing video dataset recorded with conventional cameras to synthetic event data. This unlocks the use of a virtually unlimited number of existing video datasets for training networks designed for real event data. We evaluate our method on two relevant vision tasks, ie, object recognition and semantic segmentation, and show that models trained on synthetic events have several benefits:(i) they generalize well to real event data, even in scenarios where standard-camera images are blurry or overexposed, by inheriting the outstanding properties of event cameras;(ii) they can be used for fine-tuning on real data to improve over state-of-the-art for both classification and semantic segmentation.

### ESIM: an open event camera simulator

H Rebecq, D Gehrig, D Scaramuzza - Conference on robot learning, 2018

Event cameras are revolutionary sensors that work radically differently from standard cameras. Instead of capturing intensity images at a fixed rate, event cameras measure changes of intensity asynchronously, in the form of a stream of events, which encode per-pixel brightness changes. In the last few years, their outstanding properties (asynchronous sensing, no motion blur, high dynamic range) have led to exciting vision applications, with very low-latency and high robustness. However, these sensors are still scarce and expensive to get, slowing down progress of the research community. To address these issues, there is a huge demand for cheap, high-quality synthetic, labeled event for algorithm prototyping, deep learning and algorithm benchmarking. The development of such a simulator, however, is not trivial since event cameras work fundamentally differently from frame-based cameras. We present the first event camera simulator that can generate a large amount of reliable event data. The key component of our simulator is a theoretically sound, adaptive rendering scheme that only samples frames when necessary, through a tight coupling between the rendering engine and the event simulator. We release an open source implementation of our simulator.

### Video to events: Recycling video datasets for event cameras
D Gehrig, M Gehrig, J Hidalgo-Carrió, D Scaramuzza - In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020

Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous" events" instead of intensity frames. They offer significant advantages with respect to conventional cameras: high dynamic range (HDR), high temporal resolution, and no motion blur. Recently, novel learning approaches operating on event data have achieved impressive results. Yet, these methods require a large amount of event data for training, which is hardly available due the novelty of event sensors in computer vision research. In this paper, we present a method that addresses these needs by converting any existing video dataset recorded with conventional cameras to synthetic event data. This unlocks the use of a virtually unlimited number of existing video datasets for training networks designed for real event data. We evaluate our method on two relevant vision tasks, ie, object recognition and semantic segmentation, and show that models trained on synthetic events have several benefits:(i) they generalize well to real event data, even in scenarios where standard-camera images are blurry or overexposed, by inheriting the outstanding properties of event cameras;(ii) they can be used for fine-tuning on real data to improve over state-of-the-art for both classification and semantic segmentation.

### Event camera simulator improvements via characterized parameters

D Joubert, A Marcireau, N Ralph, A Jolley - Frontiers in Neuroscience, 2021
It has been more than two decades since the first neuromorphic Dynamic Vision Sensor (DVS) sensor was invented, and many subsequent prototypes have been built with a wide spectrum of applications in mind. Competing against state-of-the-art neural networks in terms of accuracy is difficult, although there are clear opportunities to outperform conventional approaches in terms of power consumption and processing speed. As neuromorphic sensors generate sparse data at the focal plane itself, they are inherently energy-efficient, data-driven, and fast. In this work, we present an extended DVS pixel simulator for neuromorphic benchmarks which simplifies the latency and the noise models. In addition, to more closely model the behaviour of a real pixel, the readout circuitry is modelled, as this can strongly affect the time precision of events in complex scenes. Using a dynamic variant of the MNIST dataset as a benchmarking task, we use this simulator to explore how the latency of the sensor allows it to outperform conventional sensors in terms of sensing speed.

### Accurate event simulation using high-speed video

X Mou, K Feng, A Yi, S Wang, H Chen, X Hu, M Guo… - Electronic Imaging, 2022

Event sensing is a novel modality which is solely sensitive to changes of information. This redundancy reduction can be utilized to achieve high temporal resolution, reduce power consumption, simplify algorithms etc. The hardware-software co-design of event sensors and algorithms requires early simulation of the sensor system. It has been shown that high-speed video is well suited to derive such event data for temporal contrast based event sensors, but the simulators published so far neglect phenomena such as readout latency or refractory period. This paper presents ongoing modeling activities at OmniVision Technologies.

### Event-based camera simulation wrapper for arcade learning environment
C Rizzo, C Schuman, J Plank - Proceedings of the International Conference on Neuromorphic Systems, 2022

Event-based cameras are cameras with high dynamic range that measure changes in light intensity at each pixel instead of capturing frames like traditional cameras. There are several event-based camera simulation software libraries that can convert videos or collections of frames to a stream of simulated camera events. To the authors’ knowledge, with the exception of vehicle control projects, there are no software libraries that simulate event-based camera activity online during an agent’s training phase on other various control applications. This work introduces ALE_EBC, a software wrapper around the Arcade Learning Environment that converts game frames to simulated event-based camera event streams, allowing agents to be trained on a variety of control-based Atari games through the lens of a neuromorphic camera.

### Event camera simulator design for modeling attention-based inference architectures

MJH Pantho, JM Mbongue, P Bhowmik, C Bobda - Journal of Real-Time Image Processing, 2022

In recent years, there has been a growing interest in realizing methodologies to integrate more and more computation at the level of the image sensor. The rising trend has seen an increased research interest in developing novel event cameras that can facilitate CNN computation directly in the sensor. However, event-based cameras ca be expensive, limiting performance exploration on high-level models and algorithms. This paper presents an event camera simulator that can be a potent tool for hardware design prototyping, parameter optimization, attention-based innovative algorithm development, and benchmarking. The proposed simulator implements a distributed computation model to identify relevant regions in an image frame. Our simulator’s relevance computation model is realized as a collection of modules and performs computations in parallel. The distributed computation model is configurable, making it highly useful for design space exploration. The Rendering engine of the simulator samples frame-regions only when there is a new event. The simulator closely emulates an image processing pipeline similar to that of physical cameras. Our experimental results show that the simulator can effectively emulate event vision with low overheads

### v2e: From video frames to realistic DVS events

Y Hu, SC Liu, T Delbruck - Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021

To help meet the increasing need for dynamic vision sensor (DVS) event camera data, this paper proposes the v2e toolbox that generates realistic synthetic DVS events from intensity frames. It also clarifies incorrect claims about DVS motion blur and latency characteristics in recent literature. Unlike other toolboxes, v2e includes pixel-level Gaussian event threshold mismatch, finite intensity-dependent bandwidth, and intensity-dependent noise. Realistic DVS events are useful in training networks for uncontrolled lighting conditions. The use of v2e synthetic events is demonstrated in two experiments. The first experiment is object recognition with N-Caltech 101 dataset. Results show that pretraining on various v2e lighting conditions improves generalization when transferred on real DVS data for a ResNet model. The second experiment shows that for night driving, a car detector trained with v2e events shows an average accuracy improvement of 40% compared to the YOLOv3 trained on intensity frames.

### DVS-Voltmeter: Stochastic process-based event simulator for dynamic vision sensors

S Lin, Y Ma, Z Guo, B Wen - European Conference on Computer Vision, 2022

Recent advances in deep learning for event-driven applications with dynamic vision sensors (DVS) primarily rely on training over simulated data. However, most simulators ignore various physics-based characteristics of real DVS, such as the fidelity of event timestamps and comprehensive noise effects. We propose an event simulator, dubbed DVS-Voltmeter, to enable high-performance deep networks for DVS applications. DVS-Voltmeter incorporates the fundamental principle of physics - (1) voltage variations in a DVS circuit, (2) randomness caused by photon reception, and (3) noise effects caused by temperature and parasitic photocurrent - into a stochastic process. With the novel insight into the sensor design and physics, DVS-Voltmeter generates more realistic events, given high frame-rate videos. Qualitative and quantitative experiments show that the simulated events resemble real data. The evaluation on two tasks, i.e., semantic segmentation and intensity-image reconstruction, indicates that neural networks trained with DVS-Voltmeter generalize favorably on real events against state-of-the-art simulators.

### V2CE: Video to Continuous Events Simulator

Zhongyang Zhang; Shuyang Cui; Kaidong Chai; Haowen Yu; Subhasis Dasgupta; Upal Mahbub
2024 IEEE International Conference on Robotics and Automation (ICRA),

Dynamic Vision Sensor (DVS)-based solutions have recently garnered significant interest across various computer vision tasks, offering notable benefits in terms of dynamic range, temporal resolution, and inference speed. However, as a relatively nascent vision sensor compared to Active Pixel Sensor (APS) devices such as RGB cameras, DVS suffers from a dearth of ample labeled datasets. Prior efforts to convert APS data into events often grapple with issues such as a considerable domain shift from real events, the absence of quantified validation, and layering problems within the time axis. In this paper, we present a novel method for video-to-events stream conversion from multiple perspectives, considering the specific characteristics of DVS. A series of carefully designed losses helps enhance the quality of generated event voxels significantly. We also propose a novel local dynamic-aware timestamp inference strategy to accurately recover event timestamps from event voxels in a continuous fashion and eliminate the temporal layering problem. Results from rigorous validation through quantified metrics at all stages of the pipeline establish our method unquestionably as the current state-of-the-art (SOTA). The code can be found at bit.ly/v2ce.

### Physical-Based Event Camera Simulator

H Han, J Lyu, J Li, H Wei, C Li, Y Wei, S Chen, X Ji ECCV 2024

Existing event camera simulators primarily focus on the process of generating video events and often overlook the entire optical path in real-world camera systems. To address this limitation, we propose a novel Physical-based Event Camera Simulator (PECS), which is able to generate a high-fidelity realistic event stream by directly interfacing with the 3D scene. Our PECS features a lens simulation block for accurate light-to-sensor chip replication and a multispectral rendering module for precise photocurrent generation. We present two spatiotemporal event metrics to assess the similarity between simulated and actual camera events. Experimental results demonstrate that our PECS outperforms four state-of-the-art simulators by a large margin in terms of event-based signal fidelity. We integrate our PECS into the UE platform to generate extensive multi-task synthetic datasets and evaluate its effectiveness in downstream vision tasks (eg, video reconstruction). Our open-source code is available at https://github. com/lanpokn/PECS_ trail_version.

